## Reading Response3 :open_book:

### Situations
1. Weather Forecast
Weather forecasting is a system that has been used for a long time based on predictable data. Meteorological satellites and weather stations collect data and analyze weather patterns. People then see the weather forecast in TV news or check weather app to decide what to wear or how to travel for the day. And weather anomalies such as typhoons and earthquakes are also predicted and weather forecasts give warnings so that humans can better prepare for natural disasters.

2. World Cup Predictions
I remember during 2018 World Cup (I search for World Cup prediction and find prediction of 2022 World Cup has already come out), many companies and platforms such as Google, Baidu, Microsoft and Goldman Sachs have reported their prediction of the competition results. I learn from google that Google's World Cup predictions are based on Opta Sports' tournament data. And Baidu searched 37,000 matches from 987 teams around the world over the past five years, and worked with some lottery sites to import prediction data from the betting market to build the prediction model. Later the prediction results were mainly spread in betting market and also used in sports and entertainment news. 

3. Health Predictions
Health apps are popular along with the developing smart hardware industry. Wearable smart health devices help collect human health data such as heart rate, weight, blood sugar, exercise and steps, sleep and other conditions. If this data is accurate and comprehensive enough, and there are chronic disease prediction models that can be formed into algorithms, perhaps in the future your device will alert your body to the risk of developing a chronic disease. (I emphasize chronic disease becase I couldn't find a preciously predicted acute disease example.)


### Collective average vs individual fate
Hannah Fry writes about it very close to the beginning of his article, "The trouble comes when you try to go the other way—to learn something about us as individuals from how we behave as a collective." She values the collective character of the data based on large numbers of people, but shows more concerns about the reverse. Also, there's a more vivid example of how "young black men between the ages of 14 and 21" are treated in The Crime Machine. With the collected data and results, they confirmed the "maximum certainty" which would then be expanded a little and cover the whole group. The individuals that outside the majority but inside the group are, thus, “obliterated” the identity of the individual. So the logic holds that collective average can somehow represent the group because there are more people eligible in the group than the not eligible ones. The reverse logic, however, if the individual is the minority one, then he is 100% not eligible for the collective average. Now the object is the individual and inside himself, the proportion of the feature presence is 100-0. Thus, we trust in the conclusive features from big data, but never ignore the diversity of individuals. We might say: one is a member of the group, but not: one is the group.


### "a low crime rate and high police activity"
When I heard about what's behind the decreasing crime rate and high police activity, I started to think about the data itself and its results. The situation happened in the NYC police department is the consequence that the data collectors (the cops who are required to report the crimes) know what the results that their data would lead to, thus, they started to change data to fit the expected results. What leads to this messy "order" about dealing with data? When data corresponds to results, collectors, once understand the measurements, would start thinking about, falsifying and subjectively adjusting it. It is not an incentive because the collectors somehow have the freedom of changing the foundations. It is challenging for people to control in their own hands what can be falsified to determine their own fate. Thus, I feel like the bug of the NYPD was that they didn't isolate the recorder and the analyst? The collectors don't understand how the data would be analyzed, then they have no reason to change the database. Or make judging measurements complex and humanly unmeasurable, beyond what the collectors can figure out? Would this be the proper way that data be collected and analyzed if related to human beings?


### "Minority Report" 
:speak_no_evil: i know it's too much, the last one is less about data, just some personal doubts

The fiction actually mentions little about data directly. How the data is collected and put into application is concluded in only one sentence that words of the three idiots are turned to "visual symbols, transcribed on conventional punchcards, and ejected into various coded slots." Main part of it is people's attitude and actions towards the results from the data. Actually, I was confused by the cause and effect relationship in the story. Anderton and Leopold Kaplan's first encounter is due to the card, in other words, the card is the cause. However, what's on the card and cause their encounter is the result of them: Anderton kills Leopold. This confuses. WHY the card exists? If the card does not exist, then result is "hided" and cause does not exist either. Nothing can explain the card except for fate...(But I don't believe, i think its the logic gap of the fiction) Also the ending point, Mike is right, why Mike is right? Because no more results. I don't know but it feels like a satire. If you are the last one and nobody refutes you, then you are right? How about data, if for one data group, no more datapoints would come in and all the inside datapoints are eligible, would people view the results from the data group right? How about then a different data point enters? 